---
title: "GUM measurement error pilot"
author: "Jérôme Lavoué"
date: "2024-06-19"
output: word_document
---

```{r setup, include=FALSE}


knitr::opts_chunk$set(echo = TRUE)

knitr::opts_chunk$set(warning = FALSE, message = FALSE)

library(readxl)
library(ggplot2)
library(flextable)

results <- readRDS("../created data/GUM measurement error.RDS")

example <- readRDS( "../created data/GUM measurement error_ex1.RDS")
          
sim7 <- readRDS( "../created data/GUM measurement error_sim7.RDS")

```

## Introduction

This document summarizes a short  simulation effort aiming at illustrating 2 approaches to deal with measurement error in the statistical evaluation of measurement data. 

True exposure levels to airborne chemicals are often assumed to reasonably follow the lognormal distribution. When performing risk assessment, best practice recommends collecting 6-12 samples from the workplace to estimate metrics such as the 95th percentile or exceedance fraction of the OEL. The small sample sizes mentioned above added to the typically large day-to-day variability creates large uncertainty in this process. In addition, the measurements are themselves subject to error potentially introduced during the sampling and analysis phases of the measurement process.

Sampling and analysis (S&A) errors are often assumed to be normally distributed with mean zero, while environmental variability, as mentionned above, is best represented by a lognormal distribution. This combination of two different distribution has represented a challenge in studying the impact of S&A error on the final estimates of exposure metrics and their uncertainty.

Two landmark documents have studied the impact of measurement error on the statistical evaluation of exposure data. [Grzebyk and sandino](https://www.inrs.fr/media.html?refINRS=ND%202231) in France and [Nicas, Simmons and Spear](https://www.tandfonline.com/doi/abs/10.1080/15298669191365199) in the US used slightly different approximations to demonstrate that S&A error below a CV of 30% has a negligible impact on the final estimate of the exposure metric for typical workplace GSD.

Both teams had to use simplifying assumptions to make their conclusions, and they didn'T directly evaluate the impact on risk decisions. In recent decades advances in computing power have let to the rapid spread of Bayesian methods in industrial hygiene and other fields. Bayesian methods are very flexible, and indeed can be easily used to incorporate S&A error in the interpretation of exposure levels.

An Bayesian S&A error model has been developed by our team at University of Montreal during the [Webexpo project](https://www.irsst.qc.ca/en/publications-tools/publication/i/101066/n/webexpo). 

In parallel, the [supplement 1 to the ISO Guide to the expression of uncertainty in measurement (GUM)](https://www.iso.org/standard/50462.html) provides a framework to propagate measurement error in the final estimate of a statistical parameter using Monte Carlo simulation. 

This document will compare the performance of the Bayesian model to the GUM model for industrial hygiene data interpretation in the context of a simple simulation study.

## Webexpo Bayesian S&A error model

The model is described in details in the scientific reports linked above.

In essence: 

true_concentration is what really happens in the workplace, it is assumed to be lognormal with unknown geometric mean (GM) and geometric standard deviation (GSD).

Mathematically :  

* ln(true_concentration) ~ Normal( ln(GM ) , ln(GSD) )

observed_concentration is what comes out of the laboratory. For a particular value of true_concentration, the observed_concentration is assumed to be equal to the true value plus an error term, which is traditionally assumed to be normally distributed with mean 0 and a known coefficient of variation (CV). 

Mathematically : 

* observed_concentration = true_concentration + error

* error ~ Normal(0,CV)
                
As an example, for an expanded uncertainty of 50%, the CV would be ~25%.

During the analysis of a sample with this model, one would supply the observed values as well as the assumed CV, and the Bayesian engine would return posterior samples for all metrics of interest. The posterior sample represent the estimated uncertainty distribution of the parameter of interest. It is typical to use the median of the posterior sample as the point estimate, and, e.g., the 70% quantile of the posterior sample as a 70% upper confidence limit.

as an insight of the underlying principle of the bayesian error model, let's imagine exposure data was normally distributed, with mean AM and a CV = CVenvironment.

Then we would have : 

* true_concentration ~ Normal( AM , CVenvironment )
* observed_concentration = true_concentration + error
* error ~ Normal(0,CV)

With such a model, the combination is easy : observed_concentration ~ Normal( AM, sqrt(CV^2 + CVenvironment^2) )

The observed concentrations follow a normal distribution with the same mean as the true concentrations, but they have a larger variance, the sum of the environmental and S&A variances.

It is obvious in this simple example that the consequence of failing to account for S&A error would cause an overestimation of the true environmental variability, since the observed variability would also contain the noise caused by measurement error.

A Bayesian error model in this case would estimate the true environmental variability knowing that some part of the observed variance comes from the S&A error.

## GUM approach

This is the approach described by email by Robert Emond in an email from June 6th, 202 as understood (possibly misunderstood) by Jérôme Lavoué

The steps to estimate parameters and or upper limits is as follows: 

    
* sample from a normal distribution around the observed values
* calculate a metric with the new dataset (GM, GSD, 95th percentile, UTL95,70)
* repeat a lot of times - result is a lot of values of the metric
* report the average value of the metric across the many repetitions 
    
If we go back to the simplification mentioned above, it seems that this approach, through adding random noise to the observed data, would (in the case of the normal simplification), result in a quantity with mean the true exposure mean and variance the sum of the true environmental variance plus the S&E noise (the distribution of the observed concentrations ), plus another time the S&E noise ( addition of random noise to the observed concentration in the GUM approach).     
    

For the practical implementation of the GUM approach, I used the frequentist estimation methods for the 95th percentile tolerance limit as presented in [Selvin et al](https://www.tandfonline.com/doi/abs/10.1080/15298668791384445). The k values to calculate a 70% upper confidence limit on the 95th percentile (UTL95,70) was determined with the R package named ["tolerance"](https://cran.r-project.org/web/packages/tolerance/index.html), which closely replicates the values found in the EN689 document.

## Example using both approaches

The vector of true concentrations is : `r signif(example$true_data,2)`

It was generated from a lognormal distribution with 95th percentile=100 and GSD=2.5

The vector of observed concentration was created by adding normal random noise to the true concentrations. The observed concentrations would be what is accessible to the industrial hygienist. We selected a value for S&E error of 25%, which correspond to an expanded uncertainty of 50%.

The vector of observed concentrations is : `r signif(example$observed_data,2)`

Table 1 below shows the results of the determination of 4 metrics : gm, gsd, 95th percentile (p95) and the 70% upper confidence limit on the 95th percentile (P95_ucl). These were calculated using both a Bayesian and Frequentist framework for three conditions : 

* Ideal condition : the analysis was performed on the true concentrations, i.e, if there was no measurement error at all
* Naive condition : the analysis was performed on the observed concentrations, but without any provision for measurement error in the estimation process. This represents what is done by most tools today.
* ME condition :  the analysis was performed on the observed concentrations, this time with provision for measurement error: The Bayesian measurement error model for the Bayesian analysis and the GUM approach for the Frequentist analysis.

<br>

\newpage

```{r table 1, warning=FALSE, echo=FALSE}


T1 <- example$results[-1,]

ft <- flextable(T1)

ft <- autofit(ft)

ft <- align( ft, j = 2:5, align = "center", part = "all" )

ft <- set_caption(ft, "Table 1 : Applying 2 measurement error approaches to one sample", style = "strong")

ft <- colformat_double(ft, j = c(2:5), digits = 1)

ft <- footnote( ft ,
                i = 1 ,
                j = 1 ,
                value = as_paragraph("Ideal : analysis performed on true concentrations with standard approach, Naive : analysis performed on observed concentrations with standard approach, me : analysis performed on observed concentrations with measurement error approach (Bayesian or GUM)"),
                ref_symbols = "a", part = "header")


ft <- footnote( ft ,
                i = 1 ,
                j = 1 ,
                value = as_paragraph("_b : Bayesian method, _f : frequentist method"),
                ref_symbols = "b", part = "header")


ft <- hline(ft, i = c(2,4) , j= NULL, border = NULL, part = "body")


myrelativewidth <- c(1.3,1,1,1,1)

mytotalwidth <- 5


ft <- width(ft, width= myrelativewidth*mytotalwidth/sum(myrelativewidth), unit = "in")

ft <- fontsize(ft, size = 12 , part="all")


ft

```

<br>

Observations from T1 : 

* GM seems little impacted
* The naive approach clearly shows a larger variability (expected as we observed environmental variability as well as S&E noise )
* For GSD, the GUM approach to measurement error further increases the estimate compared to naive approach (expected since normal noise is further added to the observations in the Monte Carlo procedure)
* For GSD, the Bayesian approach decreases the estimate compared to naive approach (expected since the Bayesian model takes the S&E noise into account when trying to estimate the variability of the true concentrations)

* Overall, for this particular example, the Bayesian ME approach seems to get results closer to the ideal results compared to the GUM approach, but not always much closer.    
    
\newpage

## Limited simulation study
    
    
The previous example illustrate the general behavior of both measurement error approaches, but doesn't really provide useful information as to their overall performance. For example, sometimes, the random noise coming from the S&A error might actually decrease observed variability due to chance, and in that particular case, the GUM approach would yield variability estimates closer to the ideal values than the Bayesian approach. It is therefor important to observe the behavior of the 2 approaches across a large number of random samples.

To get a little bit more insight we performed a simulation study for 6 scenarios of sample size and true variability : GSD = 1.5 or 2.5 , n=3 or 6 or 9. We kept the true 95th percentile at 100. For each scenario we generated 10000 datasets of true and observed concentrations using an expanded S&E uncertainty CV of 50%. For each simulated dataset we calculated the same metrics as shown in the example above.

To summarize the results across the 10000 iterations we calculated, for gm, gsd and P95, the bias, precision and rmse of the estimates. For the 70% UCL on the 95th percentile we calculated the coverage of that UCL i.e. the proportion of time the calculated UCL was indeed greater than the true P95 out of the of 10000 iterations.

The stability of the simulation was assessed by looking at the variability of the results across 5 repeats of the entire simulation effort. To limit the amount of information presented in this document, we only show the mean results across the 5 repetitions as the standard deviations were always very small (<1%). They are however available in the raw data.

Overall this took approximate 30h of computing time on a modern desktop computer.

\newpage

### results

#### geometric mean

Table 2a and b below presents the results for the estimation of the geometric mean for GSDs of 2.5 and 1.5. All metrics are relative to the true value of the parameter. 

In all tables in the following sections, the first 2 lines correspond to the either Bayesian (the Expostats Bayesian analysis) or traditional frequentist analysis of the true data. The next 2 lines correspond to the naive analysis of the observed data (i.e not implementing any measurement error approach), and the last 2 lines correspond to the analysis of the observed data with the measurement error approach (Bayesian or GUM).

So, a focus on the first 2 lines would allow to compare Bayesian vs traditional frequentist analysis of the true data. Focus on lines 3/4 to 5/6 would compare the naive vs ME approach. And finally a focus on the last 2 lines would allow to compare the respective performance of the Bayesian vs. GUM approach to measurement error.


<br>

```{r table 2a, warning=FALSE, echo=FALSE}


T2a <- results$bias$n3_2.5[,1:2]


#BIAS
T2a <- cbind(T2a, results$bias$n6_2.5[,2],
                results$bias$n9_2.5[,2])

#PRECISION
T2a <- cbind(T2a, results$precision$n3_2.5[,2],
                results$precision$n6_2.5[,2],
                results$precision$n9_2.5[,2])

#RMSE
T2a <- cbind(T2a, results$rmse$n3_2.5[,2],
                results$rmse$n6_2.5[,2],
                results$rmse$n9_2.5[,2])


colnames(T2a) <- c("Approach","bias_n3_2.5","bias_n6_2.5","bias_n9_2.5",
                  "precision_n3_2.5","precision_n6_2.5","precision_n9_2.5",
                  "rmse_n3_2.5","rmse_n6_2.5","rmse_n9_2.5")

ft <- flextable(T2a)

ft <- autofit(ft)


## FIRST LINES

ft <- set_header_labels( ft, 
                            values = list(Approach = "Approach",
                                          bias_n3_2.5 = "n=3",
                                          bias_n6_2.5 = "n=6",
                                          bias_n9_2.5 = "n=9",
                                          precision_n3_2.5 = "n=3",
                                          precision_n6_2.5 = "n=6",
                                          precision_n9_2.5 = "n=9",
                                          rmse_n3_2.5 = "n=3",
                                          rmse_n6_2.5 = "n=6",
                                          rmse_n9_2.5 = "n=9"))

                                          
# metric
ft <- add_header_row( ft , top = TRUE, 
                          values = c("", rep(c("Bias","Precision","RMSE"),c(3,3,3))))

    
ft <- merge_at( ft , i = 1, j = c(2:4), part = "header")
ft <- merge_at( ft , i = 1, j = c(5:7), part = "header")
ft <- merge_at( ft , i = 1, j = c(8:10), part = "header")
    
        
##  rest


ft <- align( ft, j = 2:10, align = "center", part = "all" )

ft <- set_caption(ft, "Table 2a : Performance metrics for GM - GSD = 2.5", style = "strong")

ft <- colformat_double(ft, j = c(2:10), digits = 1)

ft <- footnote( ft ,
                i = 1 ,
                j = c(2,5,8) ,
                value = as_paragraph("all metrics are relative to the true value of the parameter and expressed in %"),
                ref_symbols = "a", part = "header")


ft <- vline(ft, i = NULL , j= c(1,4,7), border = NULL, part = "body")

ft <- hline(ft, i = c(2,4) , j= NULL, border = NULL, part = "body")

myrelativewidth <- c(1.3,rep(1,9))

mytotalwidth <- 7


ft <- width(ft, width= myrelativewidth*mytotalwidth/sum(myrelativewidth), unit = "in")

ft <- fontsize(ft, size = 12 , part="all")

    

ft

```

<br>

```{r table 2b, warning=FALSE, echo=FALSE}


T2b <- results$bias$n3_1.5[,1:2]


#BIAS
T2b <- cbind(T2b, results$bias$n6_1.5[,2],
                results$bias$n9_1.5[,2])

#PRECISION
T2b <- cbind(T2b, results$precision$n3_1.5[,2],
                results$precision$n6_1.5[,2],
                results$precision$n9_1.5[,2])

#RMSE
T2b <- cbind(T2b, results$rmse$n3_1.5[,2],
                results$rmse$n6_1.5[,2],
                results$rmse$n9_1.5[,2])


colnames(T2b) <- c("Approach","bias_n3_1.5","bias_n6_1.5","bias_n9_1.5",
                  "precision_n3_1.5","precision_n6_1.5","precision_n9_1.5",
                  "rmse_n3_1.5","rmse_n6_1.5","rmse_n9_1.5")

ft <- flextable(T2b)

ft <- autofit(ft)


## FIRST LINES

ft <- set_header_labels( ft, 
                            values = list(Approach = "Approach",
                                          bias_n3_1.5 = "n=3",
                                          bias_n6_1.5 = "n=6",
                                          bias_n9_1.5 = "n=9",
                                          precision_n3_1.5 = "n=3",
                                          precision_n6_1.5 = "n=6",
                                          precision_n9_1.5 = "n=9",
                                          rmse_n3_1.5 = "n=3",
                                          rmse_n6_1.5 = "n=6",
                                          rmse_n9_1.5 = "n=9"))

                                          
# metric
ft <- add_header_row( ft , top = TRUE, 
                          values = c("", rep(c("Bias","Precision","RMSE"),c(3,3,3))))

    
ft <- merge_at( ft , i = 1, j = c(2:4), part = "header")
ft <- merge_at( ft , i = 1, j = c(5:7), part = "header")
ft <- merge_at( ft , i = 1, j = c(8:10), part = "header")
    
        
##  rest


ft <- align( ft, j = 2:10, align = "center", part = "all" )

ft <- set_caption(ft, "Table 2b : Performance metrics for GM - GSD = 1.5", style = "strong")

ft <- colformat_double(ft, j = c(2:10), digits = 1)

ft <- footnote( ft ,
                i = 1 ,
                j = c(2,5,8) ,
                value = as_paragraph("all metrics are relative to the true value of the parameter and expressed in %"),
                ref_symbols = "a", part = "header")


ft <- vline(ft, i = NULL , j= c(1,4,7), border = NULL, part = "body")

ft <- hline(ft, i = c(2,4) , j= NULL, border = NULL, part = "body")
    
myrelativewidth <- c(1.3,rep(1,9))

mytotalwidth <- 7


ft <- width(ft, width= myrelativewidth*mytotalwidth/sum(myrelativewidth), unit = "in")

ft <- fontsize(ft, size = 12 , part="all")


ft

```

<br>

Table 2c and d below presents the same results as above but using different performance metrics : median difference, median absolute deviation (MAD), and RMSLE (RMSE calculated in the log scale). MAD is a robust measure of the variability of  the estimates, while RMSLE is a measure of the overal accuracy of the estimates (combining bias and precision).


```{r table 2c, warning=FALSE, echo=FALSE}


T2c <- results$me$n3_2.5[,1:2]


#me
T2c <- cbind(T2c, results$me$n6_2.5[,2],
                results$me$n9_2.5[,2])

#PRECISION
T2c <- cbind(T2c, results$mad$n3_2.5[,2],
                results$mad$n6_2.5[,2],
                results$mad$n9_2.5[,2])

#RMSE
T2c <- cbind(T2c, results$rmsle$n3_2.5[,2],
                results$rmsle$n6_2.5[,2],
                results$rmsle$n9_2.5[,2])


colnames(T2c) <- c("Approach","me_n3_2.5","me_n6_2.5","me_n9_2.5",
                  "mad_n3_2.5","mad_n6_2.5","mad_n9_2.5",
                  "rmsle_n3_2.5","rmsle_n6_2.5","rmsle_n9_2.5")

ft <- flextable(T2c)

ft <- autofit(ft)


## FIRST LINES

ft <- set_header_labels( ft, 
                            values = list(Approach = "Approach",
                                          me_n3_2.5 = "n=3",
                                          me_n6_2.5 = "n=6",
                                          me_n9_2.5 = "n=9",
                                          mad_n3_2.5 = "n=3",
                                          mad_n6_2.5 = "n=6",
                                          mad_n9_2.5 = "n=9",
                                          rmsle_n3_2.5 = "n=3",
                                          rmsle_n6_2.5 = "n=6",
                                          rmsle_n9_2.5 = "n=9"))

                                          
# metric
ft <- add_header_row( ft , top = TRUE, 
                          values = c("", rep(c("Median error","MAD","RMSLE"),c(3,3,3))))

    
ft <- merge_at( ft , i = 1, j = c(2:4), part = "header")
ft <- merge_at( ft , i = 1, j = c(5:7), part = "header")
ft <- merge_at( ft , i = 1, j = c(8:10), part = "header")
    
        
##  rest


ft <- align( ft, j = 2:10, align = "center", part = "all" )

ft <- set_caption(ft, "Table 2c : Alternative performance metrics for GM - GSD = 2.5", style = "strong")

ft <- colformat_double(ft, j = c(2:10), digits = 2)

ft <- footnote( ft ,
                i = 1 ,
                j = c(2) ,
                value = as_paragraph("True value of GM is 22.15"),
                ref_symbols = "a", part = "header")


ft <- vline(ft, i = NULL , j= c(1,4,7), border = NULL, part = "body")

ft <- hline(ft, i = c(2,4) , j= NULL, border = NULL, part = "body")
    
    
myrelativewidth <- c(1.3,rep(1,9))

mytotalwidth <- 7


ft <- width(ft, width= myrelativewidth*mytotalwidth/sum(myrelativewidth), unit = "in")

ft <- fontsize(ft, size = 12 , part="all")


ft

```

<br>

```{r table 2d, warning=FALSE, echo=FALSE}


T2d <- results$me$n3_1.5[,1:2]


#me
T2d <- cbind(T2d, results$me$n6_1.5[,2],
                results$me$n9_1.5[,2])

#PRECISION
T2d <- cbind(T2d, results$mad$n3_1.5[,2],
                results$mad$n6_1.5[,2],
                results$mad$n9_1.5[,2])

#RMSE
T2d <- cbind(T2d, results$rmsle$n3_1.5[,2],
                results$rmsle$n6_1.5[,2],
                results$rmsle$n9_1.5[,2])


colnames(T2d) <- c("Approach","me_n3_1.5","me_n6_1.5","me_n9_1.5",
                  "mad_n3_1.5","mad_n6_1.5","mad_n9_1.5",
                  "rmsle_n3_1.5","rmsle_n6_1.5","rmsle_n9_1.5")

ft <- flextable(T2d)

ft <- autofit(ft)


## FIRST LINES

ft <- set_header_labels( ft, 
                            values = list(Approach = "Approach",
                                          me_n3_1.5 = "n=3",
                                          me_n6_1.5 = "n=6",
                                          me_n9_1.5 = "n=9",
                                          mad_n3_1.5 = "n=3",
                                          mad_n6_1.5 = "n=6",
                                          mad_n9_1.5 = "n=9",
                                          rmsle_n3_1.5 = "n=3",
                                          rmsle_n6_1.5 = "n=6",
                                          rmsle_n9_1.5 = "n=9"))

                                          
# metric
ft <- add_header_row( ft , top = TRUE, 
                          values = c("", rep(c("Median error","MAD","RMSLE"),c(3,3,3))))

    
ft <- merge_at( ft , i = 1, j = c(2:4), part = "header")
ft <- merge_at( ft , i = 1, j = c(5:7), part = "header")
ft <- merge_at( ft , i = 1, j = c(8:10), part = "header")
    
        
##  rest


ft <- align( ft, j = 2:10, align = "center", part = "all" )

ft <- set_caption(ft, "Table 2c : Alternative performance metrics for GM - GSD = 1.5", style = "strong")

ft <- colformat_double(ft, j = c(2:10), digits = 2)

ft <- footnote( ft ,
                i = 1 ,
                j = c(2) ,
                value = as_paragraph("True value of GM is 51.3"),
                ref_symbols = "a", part = "header")


ft <- vline(ft, i = NULL , j= c(1,4,7), border = NULL, part = "body")

ft <- hline(ft, i = c(2,4) , j= NULL, border = NULL, part = "body")
    
    
myrelativewidth <- c(1.3,rep(1,9))

mytotalwidth <- 7


ft <- width(ft, width= myrelativewidth*mytotalwidth/sum(myrelativewidth), unit = "in")

ft <- fontsize(ft, size = 12 , part="all")


ft

```

<br>

*overall observations from T2a to T2d :*

* It might seem counter intuitive to see some bias for GM, whichever the approach, but this is due to the skewed distribution of the estimated parameters in this simulation study. Table 2c and 2d show that the median error is very close to 0 for GM.

* Estimation of GM seems little affected by the choice between the Bayesian and Frequentist approach, using measurement error or not. Theonly discernible  difference is the GUM approach seem to slightly decrease the GM estimate compared to the naive approach whereas the the Bayesian ME approach does the opposite.


\newpage

#### geometric standard deviation

Table 3a and b below presents the results for the estimation of the geometric standard deviation for true GSD = 2.5 and 1.5. All metrics are relative to the true value of the parameter. 


```{r table 3a, warning=FALSE, echo=FALSE}


T3a <- results$bias$n3_2.5[,c(1,4)]


#BIAS
T3a <- cbind(T3a, results$bias$n6_2.5[,4],
                results$bias$n9_2.5[,4])

#PRECISION
T3a <- cbind(T3a, results$precision$n3_2.5[,4],
                results$precision$n6_2.5[,4],
                results$precision$n9_2.5[,4])

#RMSE
T3a <- cbind(T3a, results$rmse$n3_2.5[,4],
                results$rmse$n6_2.5[,4],
                results$rmse$n9_2.5[,4])


colnames(T3a) <- c("Approach","bias_n3_2.5","bias_n6_2.5","bias_n9_2.5",
                  "precision_n3_2.5","precision_n6_2.5","precision_n9_2.5",
                  "rmse_n3_2.5","rmse_n6_2.5","rmse_n9_2.5")

ft <- flextable(T3a)

ft <- autofit(ft)


## FIRST LINES

ft <- set_header_labels( ft, 
                            values = list(Approach = "Approach",
                                          bias_n3_2.5 = "n=3",
                                          bias_n6_2.5 = "n=6",
                                          bias_n9_2.5 = "n=9",
                                          precision_n3_2.5 = "n=3",
                                          precision_n6_2.5 = "n=6",
                                          precision_n9_2.5 = "n=9",
                                          rmse_n3_2.5 = "n=3",
                                          rmse_n6_2.5 = "n=6",
                                          rmse_n9_2.5 = "n=9"))

                                          
# metric
ft <- add_header_row( ft , top = TRUE, 
                          values = c("", rep(c("Bias","Precision","RMSE"),c(3,3,3))))

    
ft <- merge_at( ft , i = 1, j = c(2:4), part = "header")
ft <- merge_at( ft , i = 1, j = c(5:7), part = "header")
ft <- merge_at( ft , i = 1, j = c(8:10), part = "header")
    
        
##  rest


ft <- align( ft, j = 2:10, align = "center", part = "all" )

ft <- set_caption(ft, "Table 3a : Performance metrics for GSD - GSD =2.5", style = "strong")

ft <- colformat_double(ft, j = c(2:10), digits = 1)

ft <- footnote( ft ,
                i = 1 ,
                j = c(2,5,8) ,
                value = as_paragraph("all metrics are relative to the true value of the parameter and expressed in %"),
                ref_symbols = "a", part = "header")


ft <- vline(ft, i = NULL , j= c(1,4,7), border = NULL, part = "body")

ft <- hline(ft, i = c(2,4) , j= NULL, border = NULL, part = "body")
    
    
myrelativewidth <- c(1.3,rep(1,9))

mytotalwidth <- 7


ft <- width(ft, width= myrelativewidth*mytotalwidth/sum(myrelativewidth), unit = "in")

ft <- fontsize(ft, size = 12 , part="all")


ft


```

<br>

```{r table 3b, warning=FALSE, echo=FALSE}


T3b <- results$bias$n3_1.5[,c(1,4)]


#BIAS
T3b <- cbind(T3b, results$bias$n6_1.5[,4],
                results$bias$n9_1.5[,4])

#PRECISION
T3b <- cbind(T3b, results$precision$n3_1.5[,4],
                results$precision$n6_1.5[,4],
                results$precision$n9_1.5[,4])

#RMSE
T3b <- cbind(T3b, results$rmse$n3_1.5[,4],
                results$rmse$n6_1.5[,4],
                results$rmse$n9_1.5[,4])


colnames(T3b) <- c("Approach","bias_n3_1.5","bias_n6_1.5","bias_n9_1.5",
                  "precision_n3_1.5","precision_n6_1.5","precision_n9_1.5",
                  "rmse_n3_1.5","rmse_n6_1.5","rmse_n9_1.5")

ft <- flextable(T3b)

ft <- autofit(ft)


## FIRST LINES

ft <- set_header_labels( ft, 
                            values = list(Approach = "Approach",
                                          bias_n3_1.5 = "n=3",
                                          bias_n6_1.5 = "n=6",
                                          bias_n9_1.5 = "n=9",
                                          precision_n3_1.5 = "n=3",
                                          precision_n6_1.5 = "n=6",
                                          precision_n9_1.5 = "n=9",
                                          rmse_n3_1.5 = "n=3",
                                          rmse_n6_1.5 = "n=6",
                                          rmse_n9_1.5 = "n=9"))

                                          
# metric
ft <- add_header_row( ft , top = TRUE, 
                          values = c("", rep(c("Bias","Precision","RMSE"),c(3,3,3))))

    
ft <- merge_at( ft , i = 1, j = c(2:4), part = "header")
ft <- merge_at( ft , i = 1, j = c(5:7), part = "header")
ft <- merge_at( ft , i = 1, j = c(8:10), part = "header")
    
        
##  rest


ft <- align( ft, j = 2:10, align = "center", part = "all" )

ft <- set_caption(ft, "Table 3a : Performance metrics for GSD - GSD =1.5", style = "strong")

ft <- colformat_double(ft, j = c(2:10), digits = 1)

ft <- footnote( ft ,
                i = 1 ,
                j = c(2,5,8) ,
                value = as_paragraph("all metrics are relative to the true value of the parameter and expressed in %"),
                ref_symbols = "a", part = "header")


ft <- vline(ft, i = NULL , j= c(1,4,7), border = NULL, part = "body")

ft <- hline(ft, i = c(2,4) , j= NULL, border = NULL, part = "body")
    
    
myrelativewidth <- c(1.3,rep(1,9))

mytotalwidth <- 7


ft <- width(ft, width= myrelativewidth*mytotalwidth/sum(myrelativewidth), unit = "in")

ft <- fontsize(ft, size = 12 , part="all")


ft


```

<br>

Table 3c and d below presents the same results as above but using different performance metrics : median difference, median absolute deviation, and rmsle. 


```{r table 3c, warning=FALSE, echo=FALSE}


T3c <- results$me$n3_2.5[,c(1,4)]


#me
T3c <- cbind(T3c, results$me$n6_2.5[,4],
                results$me$n9_2.5[,4])

#mad
T3c <- cbind(T3c, results$mad$n3_2.5[,4],
                results$mad$n6_2.5[,4],
                results$mad$n9_2.5[,4])

#rmsle
T3c <- cbind(T3c, results$rmsle$n3_2.5[,4],
                results$rmsle$n6_2.5[,4],
                results$rmsle$n9_2.5[,4])


colnames(T3c) <- c("Approach","me_n3_2.5","me_n6_2.5","me_n9_2.5",
                  "mad_n3_2.5","mad_n6_2.5","mad_n9_2.5",
                  "rmsle_n3_2.5","rmsle_n6_2.5","rmsle_n9_2.5")

ft <- flextable(T3c)

ft <- autofit(ft)


## FIRST LINES

ft <- set_header_labels( ft, 
                            values = list(Approach = "Approach",
                                          me_n3_2.5 = "n=3",
                                          me_n6_2.5 = "n=6",
                                          me_n9_2.5 = "n=9",
                                          mad_n3_2.5 = "n=3",
                                          mad_n6_2.5 = "n=6",
                                          mad_n9_2.5 = "n=9",
                                          rmsle_n3_2.5 = "n=3",
                                          rmsle_n6_2.5 = "n=6",
                                          rmsle_n9_2.5 = "n=9"))

                                          
# metric
ft <- add_header_row( ft , top = TRUE, 
                          values = c("", rep(c("Median error","MAD","RMSLE"),c(3,3,3))))

    
ft <- merge_at( ft , i = 1, j = c(2:4), part = "header")
ft <- merge_at( ft , i = 1, j = c(5:7), part = "header")
ft <- merge_at( ft , i = 1, j = c(8:10), part = "header")
    
        
##  rest


ft <- align( ft, j = 2:10, align = "center", part = "all" )

ft <- set_caption(ft, "Table 3c : Alternative performance metrics for GSD - GSD =2.5", style = "strong")

ft <- colformat_double(ft, j = c(2:10), digits = 2)

ft <- footnote( ft ,
                i = 1 ,
                j = c(2) ,
                value = as_paragraph("True value is 2.5"),
                ref_symbols = "a", part = "header")


ft <- vline(ft, i = NULL , j= c(1,4,7), border = NULL, part = "body")

ft <- hline(ft, i = c(2,4) , j= NULL, border = NULL, part = "body")
    
    
myrelativewidth <- c(1.3,rep(1,9))

mytotalwidth <- 7


ft <- width(ft, width= myrelativewidth*mytotalwidth/sum(myrelativewidth), unit = "in")

ft <- fontsize(ft, size = 12 , part="all")


ft


```

<br>

```{r table 3d, warning=FALSE, echo=FALSE}


T3d <- results$me$n3_1.5[,c(1,4)]


#me
T3d <- cbind(T3d, results$me$n6_1.5[,4],
                results$me$n9_1.5[,4])

#mad
T3d <- cbind(T3d, results$mad$n3_1.5[,4],
                results$mad$n6_1.5[,4],
                results$mad$n9_1.5[,4])

#rmsle
T3d <- cbind(T3d, results$rmsle$n3_1.5[,4],
                results$rmsle$n6_1.5[,4],
                results$rmsle$n9_1.5[,4])


colnames(T3d) <- c("Approach","me_n3_1.5","me_n6_1.5","me_n9_1.5",
                  "mad_n3_1.5","mad_n6_1.5","mad_n9_1.5",
                  "rmsle_n3_1.5","rmsle_n6_1.5","rmsle_n9_1.5")

ft <- flextable(T3d)

ft <- autofit(ft)


## FIRST LINES

ft <- set_header_labels( ft, 
                            values = list(Approach = "Approach",
                                          me_n3_1.5 = "n=3",
                                          me_n6_1.5 = "n=6",
                                          me_n9_1.5 = "n=9",
                                          mad_n3_1.5 = "n=3",
                                          mad_n6_1.5 = "n=6",
                                          mad_n9_1.5 = "n=9",
                                          rmsle_n3_1.5 = "n=3",
                                          rmsle_n6_1.5 = "n=6",
                                          rmsle_n9_1.5 = "n=9"))

                                          
# metric
ft <- add_header_row( ft , top = TRUE, 
                          values = c("", rep(c("Median error","MAD","RMSLE"),c(3,3,3))))

    
ft <- merge_at( ft , i = 1, j = c(2:4), part = "header")
ft <- merge_at( ft , i = 1, j = c(5:7), part = "header")
ft <- merge_at( ft , i = 1, j = c(8:10), part = "header")
    
        
##  rest


ft <- align( ft, j = 2:10, align = "center", part = "all" )

ft <- set_caption(ft, "Table 3d : Alternative performance metrics for GSD - GSD =1.5", style = "strong")

ft <- colformat_double(ft, j = c(2:10), digits = 2)

ft <- footnote( ft ,
                i = 1 ,
                j = c(2) ,
                value = as_paragraph("True value is 1.5"),
                ref_symbols = "a", part = "header")


ft <- vline(ft, i = NULL , j= c(1,4,7), border = NULL, part = "body")

ft <- hline(ft, i = c(2,4) , j= NULL, border = NULL, part = "body")
    
    
myrelativewidth <- c(1.3,rep(1,9))

mytotalwidth <- 7


ft <- width(ft, width= myrelativewidth*mytotalwidth/sum(myrelativewidth), unit = "in")

ft <- fontsize(ft, size = 12 , part="all")


ft


```

<br>

*overall observations from T3a to T23 :*

It might be useful to consider here separately tables T3a and T3c, which correspond to true environmental variability quite close to the center of the expostats prior for exposure variability : with such variability we expect expostats to perform better than the frequentist for the estimation of GSD. On the other hand, T3b and T3d correspond to a true environmental variability quite far from the center of the expostats prior  : with such variability we expect the frequentist estimation of GSD to perform better than expostats, as expostats will pull the estimates towards more "reasonable" values, especially for n=3, but even to some extent for n=6.

overall picture for T3a and T3c

* The Bayesian advantage for true GSDs close to 2.5 is obvious in the "ideal" lines of the tables, with better precision (traditional precision and MAD) and overall performance (RMSE and RMSLE).
* The "inflation" of GSD effect expected in the naive vs ideal approach is there but fairly low ( 4% positive bias for naive compared to ideal approach for both the Bayesian and frequentist approaches). Similarly the "double inflation" associated with the GUM approach is also visible ( +4% from naive to ME), and the Bayesian approach to ME also shows the "deflation" towards the ideal value. The median error results, less affected by the skewed distribution of estimates, show a similar pattern in the evolution from ideal to naive to ME. However in absolute term, the medians, being shifted to lower values compared to bias, suggest here that the GUM approach would lead to a slight overestimation and the Bayesian to a slight underestimation ( instead of larger overestimation and no bias). The bias and median error result might seem counter intuitive but they both suggest low influence of S&A error. 
* In terms of precision/MAD going from ideal to naive always decrease precision, but then when going to the ME approach, the precision is re-increased with the Bayesian approach but further decreased with the GUM approach.
* Overall focusing on the last 2 lines of the tables, the RMSE/RMSLE results show that the Bayesian approach to ME is always better than the GUM approach.


overall picture for T3b and T3d

* The Bayesian disadvantage for true GSDs close to 1.5 is obvious in the "ideal" lines of the tables, with a clear positive bias (also seen in median error), especially noticeable for n=3.
* With such low variability the S&A error is more noticeable, with the frequentist biases going from ~0 to 8% to 17% from the ideal to the naive to the GUM analysis. All in all, the bayesian ME approach, handicapped by its prior, seems perform similarly to the GUM approach, with similar biases but an opposite effect of sample size ( the bayesian approach seems better for larger n, the GUM approach for lower n). The precision and overall metrics still favor the Bayesian ME approach compared to GUM, but the difference is smaller.



\newpage

#### 95th percentile

Table 4a and b below presents the results for the estimation of the 95th percentile for true GSD = 2.5 and 1.5. All metrics are relative to the true value of the parameter. 


```{r table 4a, warning=FALSE, echo=FALSE}


T4a <- results$bias$n3_2.5[,c(1,6)]


#BIAS
T4a <- cbind(T4a, results$bias$n6_2.5[,6],
                results$bias$n9_2.5[,6])

#PRECISION
T4a <- cbind(T4a, results$precision$n3_2.5[,6],
                results$precision$n6_2.5[,6],
                results$precision$n9_2.5[,6])

#RMSE
T4a <- cbind(T4a, results$rmse$n3_2.5[,6],
                results$rmse$n6_2.5[,6],
                results$rmse$n9_2.5[,6])


colnames(T4a) <- c("Approach","bias_n3_2.5","bias_n6_2.5","bias_n9_2.5",
                  "precision_n3_2.5","precision_n6_2.5","precision_n9_2.5",
                  "rmse_n3_2.5","rmse_n6_2.5","rmse_n9_2.5")

ft <- flextable(T4a)

ft <- autofit(ft)


## FIRST LINES

ft <- set_header_labels( ft, 
                            values = list(Approach = "Approach",
                                          bias_n3_2.5 = "n=3",
                                          bias_n6_2.5 = "n=6",
                                          bias_n9_2.5 = "n=9",
                                          precision_n3_2.5 = "n=3",
                                          precision_n6_2.5 = "n=6",
                                          precision_n9_2.5 = "n=9",
                                          rmse_n3_2.5 = "n=3",
                                          rmse_n6_2.5 = "n=6",
                                          rmse_n9_2.5 = "n=9"))

                                          
# metric
ft <- add_header_row( ft , top = TRUE, 
                          values = c("", rep(c("Bias","Precision","RMSE"),c(3,3,3))))

    
ft <- merge_at( ft , i = 1, j = c(2:4), part = "header")
ft <- merge_at( ft , i = 1, j = c(5:7), part = "header")
ft <- merge_at( ft , i = 1, j = c(8:10), part = "header")
    
        
##  rest


ft <- align( ft, j = 2:10, align = "center", part = "all" )

ft <- set_caption(ft, "Table 4a : Performance metrics for 95th percentile - GSD =2.5", style = "strong")

ft <- colformat_double(ft, j = c(2:10), digits = 1)

ft <- footnote( ft ,
                i = 1 ,
                j = c(2,5,8) ,
                value = as_paragraph("all metrics are relative to the true value of the parameter and expressed in %"),
                ref_symbols = "a", part = "header")


ft <- vline(ft, i = NULL , j= c(1,4,7), border = NULL, part = "body")

ft <- hline(ft, i = c(2,4) , j= NULL, border = NULL, part = "body")
    
    
myrelativewidth <- c(1.3,rep(1,9))

mytotalwidth <- 7


ft <- width(ft, width= myrelativewidth*mytotalwidth/sum(myrelativewidth), unit = "in")

ft <- fontsize(ft, size = 12 , part="all")


ft



```

<br>

```{r table 4b, warning=FALSE, echo=FALSE}


T4b <- results$bias$n3_1.5[,c(1,6)]


#BIAS
T4b <- cbind(T4b, results$bias$n6_1.5[,6],
                results$bias$n9_1.5[,6])

#PRECISION
T4b <- cbind(T4b, results$precision$n3_1.5[,6],
                results$precision$n6_1.5[,6],
                results$precision$n9_1.5[,6])

#RMSE
T4b <- cbind(T4b, results$rmse$n3_1.5[,6],
                results$rmse$n6_1.5[,6],
                results$rmse$n9_1.5[,6])


colnames(T4b) <- c("Approach","bias_n3_1.5","bias_n6_1.5","bias_n9_1.5",
                  "precision_n3_1.5","precision_n6_1.5","precision_n9_1.5",
                  "rmse_n3_1.5","rmse_n6_1.5","rmse_n9_1.5")

ft <- flextable(T4b)

ft <- autofit(ft)


## FIRST LINES

ft <- set_header_labels( ft, 
                            values = list(Approach = "Approach",
                                          bias_n3_1.5 = "n=3",
                                          bias_n6_1.5 = "n=6",
                                          bias_n9_1.5 = "n=9",
                                          precision_n3_1.5 = "n=3",
                                          precision_n6_1.5 = "n=6",
                                          precision_n9_1.5 = "n=9",
                                          rmse_n3_1.5 = "n=3",
                                          rmse_n6_1.5 = "n=6",
                                          rmse_n9_1.5 = "n=9"))

                                          
# metric
ft <- add_header_row( ft , top = TRUE, 
                          values = c("", rep(c("Bias","Precision","RMSE"),c(3,3,3))))

    
ft <- merge_at( ft , i = 1, j = c(2:4), part = "header")
ft <- merge_at( ft , i = 1, j = c(5:7), part = "header")
ft <- merge_at( ft , i = 1, j = c(8:10), part = "header")
    
        
##  rest


ft <- align( ft, j = 2:10, align = "center", part = "all" )

ft <- set_caption(ft, "Table 4b : Performance metrics for 95th percentile - GSD =1.5", style = "strong")

ft <- colformat_double(ft, j = c(2:10), digits = 1)

ft <- footnote( ft ,
                i = 1 ,
                j = c(2,5,8) ,
                value = as_paragraph("all metrics are relative to the true value of the parameter and expressed in %"),
                ref_symbols = "a", part = "header")


ft <- vline(ft, i = NULL , j= c(1,4,7), border = NULL, part = "body")

ft <- hline(ft, i = c(2,4) , j= NULL, border = NULL, part = "body")
    
    
myrelativewidth <- c(1.3,rep(1,9))

mytotalwidth <- 7


ft <- width(ft, width= myrelativewidth*mytotalwidth/sum(myrelativewidth), unit = "in")

ft <- fontsize(ft, size = 12 , part="all")


ft



```

<br>

Table 4c and d below presents the same results as above but using different performance metrics : median difference, median absolute deviation, and rmsle. 

```{r table 4c, warning=FALSE, echo=FALSE}


T4c <- results$me$n3_2.5[,c(1,6)]


#me
T4c <- cbind(T4c, results$me$n6_2.5[,6],
                results$me$n9_2.5[,6])

#mad
T4c <- cbind(T4c, results$mad$n3_2.5[,6],
                results$mad$n6_2.5[,6],
                results$mad$n9_2.5[,6])

#rmsle
T4c <- cbind(T4c, results$rmsle$n3_2.5[,6],
                results$rmsle$n6_2.5[,6],
                results$rmsle$n9_2.5[,6])


colnames(T4c) <- c("Approach","me_n3_2.5","me_n6_2.5","me_n9_2.5",
                  "mad_n3_2.5","mad_n6_2.5","mad_n9_2.5",
                  "rmsle_n3_2.5","rmsle_n6_2.5","rmsle_n9_2.5")

ft <- flextable(T4c)

ft <- autofit(ft)


## FIRST LINES

ft <- set_header_labels( ft, 
                            values = list(Approach = "Approach",
                                          me_n3_2.5 = "n=3",
                                          me_n6_2.5 = "n=6",
                                          me_n9_2.5 = "n=9",
                                          mad_n3_2.5 = "n=3",
                                          mad_n6_2.5 = "n=6",
                                          mad_n9_2.5 = "n=9",
                                          rmsle_n3_2.5 = "n=3",
                                          rmsle_n6_2.5 = "n=6",
                                          rmsle_n9_2.5 = "n=9"))

                                          
# metric
ft <- add_header_row( ft , top = TRUE, 
                          values = c("", rep(c("Median error","MAD","RMSLE"),c(3,3,3))))

    
ft <- merge_at( ft , i = 1, j = c(2:4), part = "header")
ft <- merge_at( ft , i = 1, j = c(5:7), part = "header")
ft <- merge_at( ft , i = 1, j = c(8:10), part = "header")
    
        
##  rest


ft <- align( ft, j = 2:10, align = "center", part = "all" )

ft <- set_caption(ft, "Table 4c : Alternative performance metrics for 95th percentile - GSD =2.5", style = "strong")

ft <- colformat_double(ft, j = c(2:7), digits = 0)
ft <- colformat_double(ft, j = c(8:10), digits = 2)


ft <- footnote( ft ,
                i = 1 ,
                j = c(2) ,
                value = as_paragraph("True value is 100"),
                ref_symbols = "a", part = "header")


ft <- vline(ft, i = NULL , j= c(1,4,7), border = NULL, part = "body")

ft <- hline(ft, i = c(2,4) , j= NULL, border = NULL, part = "body")
    
    
myrelativewidth <- c(1.3,rep(1,9))

mytotalwidth <- 7


ft <- width(ft, width= myrelativewidth*mytotalwidth/sum(myrelativewidth), unit = "in")

ft <- fontsize(ft, size = 12 , part="all")


ft



```

<br>

```{r table 4d, warning=FALSE, echo=FALSE}


T4d <- results$me$n3_1.5[,c(1,6)]


#me
T4d <- cbind(T4d, results$me$n6_1.5[,6],
                results$me$n9_1.5[,6])

#mad
T4d <- cbind(T4d, results$mad$n3_1.5[,6],
                results$mad$n6_1.5[,6],
                results$mad$n9_1.5[,6])

#rmsle
T4d <- cbind(T4d, results$rmsle$n3_1.5[,6],
                results$rmsle$n6_1.5[,6],
                results$rmsle$n9_1.5[,6])


colnames(T4d) <- c("Approach","me_n3_1.5","me_n6_1.5","me_n9_1.5",
                  "mad_n3_1.5","mad_n6_1.5","mad_n9_1.5",
                  "rmsle_n3_1.5","rmsle_n6_1.5","rmsle_n9_1.5")

ft <- flextable(T4d)

ft <- autofit(ft)


## FIRST LINES

ft <- set_header_labels( ft, 
                            values = list(Approach = "Approach",
                                          me_n3_1.5 = "n=3",
                                          me_n6_1.5 = "n=6",
                                          me_n9_1.5 = "n=9",
                                          mad_n3_1.5 = "n=3",
                                          mad_n6_1.5 = "n=6",
                                          mad_n9_1.5 = "n=9",
                                          rmsle_n3_1.5 = "n=3",
                                          rmsle_n6_1.5 = "n=6",
                                          rmsle_n9_1.5 = "n=9"))

                                          
# metric
ft <- add_header_row( ft , top = TRUE, 
                          values = c("", rep(c("Median error","MAD","RMSLE"),c(3,3,3))))

    
ft <- merge_at( ft , i = 1, j = c(2:4), part = "header")
ft <- merge_at( ft , i = 1, j = c(5:7), part = "header")
ft <- merge_at( ft , i = 1, j = c(8:10), part = "header")
    
        
##  rest


ft <- align( ft, j = 2:10, align = "center", part = "all" )

ft <- set_caption(ft, "Table 4d : Alternative performance metrics for 95th percentile - GSD = 1.5", style = "strong")

ft <- colformat_double(ft, j = c(2:7), digits = 0)
ft <- colformat_double(ft, j = c(8:10), digits = 2)


ft <- footnote( ft ,
                i = 1 ,
                j = c(2) ,
                value = as_paragraph("True value is 100"),
                ref_symbols = "a", part = "header")


ft <- vline(ft, i = NULL , j= c(1,4,7), border = NULL, part = "body")

ft <- hline(ft, i = c(2,4) , j= NULL, border = NULL, part = "body")
    
    
myrelativewidth <- c(1.3,rep(1,9))

mytotalwidth <- 7


ft <- width(ft, width= myrelativewidth*mytotalwidth/sum(myrelativewidth), unit = "in")

ft <- fontsize(ft, size = 12 , part="all")


ft



```

<br>

Similar to the case of GSD it seems useful here to seprate the results for true GSD = 2.5 and true GSD = 1.5.

overall picture for T3a and T3c

* focusing on the ideal case, the advantage of the expostat prior for variability appears here too, both in terms of bias and precision (and their robust equivalent).
* the naive approach, as expected since associated with increased variability, leads to an overestimation of the 95th percentile (traditional bias), with the GUM approach leading to a further overestimation, and the Bayesian approach to getting closer to the ideal case. As in the case of GSD however, the the median errors, shifted to lower values compared to bias, suggest here that the GUM approach would lead to a slightly better situation compared to the Bayesian ME approach. Precision and overall metrics still favor the Bayesian ME approach compared to GUM for the ideal, naive and ME approaches, the most striking difference shown for n=3 (double RMSE for GUM compared to Bayesian).


overall picture for T3b and T3d

* The Bayesian disadvantage for true GSDs close to 1.5 is also obvious in the "ideal" lines of the tables, with a clear positive bias (also seen in median error), especially noticeable for n=3. Interestingly the overall metrics, while now favoring the Frequentist analysis, remain close.
* The Ideal to Naive to ME evolution is again with overall metrics worsening for the GUM approach and worsening then improving for the Bayesian ME approach.
* In terms of Bias, the baysian approach remains positively biased due to the expostats prior but its extent is mitigated by increased sample size. The frequentist estimates correspod to an increasing bias from ideal to naive to GUM, more accentuated at higher sample size.
* focusing on the last 2 lines of the tables, the traditional metrics (precision/RMS) still favor the bayesian approach despite the bias, but MAD and RMSLE are closer.





\newpage

#### 95th percentile UCL

Table 5 below presents the results for the coverage of the calculated 70%UCL : ideally it should be superior equal to 70%: the 70% UCL should be above the true value at least 70% of the time. 


```{r table 5, warning=FALSE, echo=FALSE}


T5 <- results$coverage[,c(1,4,2,10,8,6,12)]

T5$n3_2.5_mean[c(1,3,5)] <- 100*c(sim7$sim[[2]]$ideal, sim7$sim[[2]]$naive, sim7$sim[[2]]$me)
T5$n6_2.5_mean[c(1,3,5)] <- 100*c(sim7$sim[[1]]$ideal, sim7$sim[[1]]$naive, sim7$sim[[1]]$me)
T5$n9_2.5_mean[c(1,3,5)] <- 100*c(sim7$sim[[5]]$ideal, sim7$sim[[5]]$naive, sim7$sim[[5]]$me)

T5$n3_1.5_mean[c(1,3,5)] <- 100*c(sim7$sim[[4]]$ideal, sim7$sim[[4]]$naive, sim7$sim[[4]]$me)
T5$n6_1.5_mean[c(1,3,5)] <- 100*c(sim7$sim[[3]]$ideal, sim7$sim[[3]]$naive, sim7$sim[[3]]$me)
T5$n9_1.5_mean[c(1,3,5)] <- 100*c(sim7$sim[[6]]$ideal, sim7$sim[[6]]$naive, sim7$sim[[6]]$me)

colnames(T5) <- c("Approach","n3_2.5","n6_2.5","n9_2.5","n3_1.5","n6_1.5","n9_1.5")

ft <- flextable(T5)

ft <- autofit(ft)


## FIRST LINES

ft <- set_header_labels( ft, 
                            values = list(Approach = "Approach",
                                          n3_2.5 = "n=3",
                                          n6_2.5 = "n=6",
                                          n9_2.5 = "n=9",
                                          n3_1.5 = "n=3",
                                          n6_1.5 = "n=6",
                                          n9_1.5 = "n=9"
                                          ))

                                          
#gsd                                          
ft <- add_header_row( ft , top = TRUE, 
                          values = c("", rep(c("GSD=2.5","GSD=1.5"),c(3,3))  ))

ft <- merge_at( ft , i = 1, j = c(2:4), part = "header")
ft <- merge_at( ft , i = 1, j = c(5:7), part = "header")



##  rest


ft <- align( ft, j = 2:7, align = "center", part = "all" )

ft <- set_caption(ft, "Table 5 : Coverage the 95th percentile 70% UCL (%)", style = "strong")

ft <- colformat_double(ft, j = c(2:7), digits = 0)

ft <- vline(ft, i = NULL , j= c(1,4), border = NULL, part = "body")
  
ft <- hline(ft, i = c(2,4) , j= NULL, border = NULL, part = "body")
  
    
myrelativewidth <- c(1.3,rep(1,6))

mytotalwidth <- 5.5


ft <- width(ft, width= myrelativewidth*mytotalwidth/sum(myrelativewidth), unit = "in")

ft <- fontsize(ft, size = 12 , part="all")


ft

```

Observations : 

* As a introduction, ideally, coverage should be 70% : with the current decision scheme comparing UTL95,70 to OEL, lower coverage would imply an impact on sensitivity to identify truly non compliant situations, but higher coverage would imply an impact on specificity, leading to failure to identify truly compliant situations.

* In the case of GSD = 2.5, when the expostats prior is close to the true value, the coverage of the 70% UCL is close to the expected 70% for all sample sizes and approaches, albeit with coverage slightly above 70% for the frequentist approach and slightly below (up to 5% below for the Bayesian approach). Regarding the measurement error analysis, the difference between the Bayesian and GUM approach is starker (min coverage for Bayesian with n=3 of 65%, vs max coverage for GUM with n=3 of 78%). 

* In the case of GSD = 1.5, when the expostats prior is far from the true value, the positive bias on variabiity estimation causes high coverage values, only partially compensated by sample size. In the case of the frequentist approach, coverage is ideal for the ideal sitation, and increased in the naive analysis (~5%), and further increases to reach similar values to the Bayesian approach in the GUM analysis. 


#### Conclusions ####

I started this analysis with a clear notion that theoretically there is an issue with the GUM approach, where the variability corresponding to S&E is counted twice.

In practice the overall picture is less clear : 

* indeed the Bayesian ME approach yield results closer to the ideal situation compared to the naive analysis
* indeed the GUM approach is seen to add further variability compared to the naive approach
* indeed in general, in terms of traditional estimation performance ( RMSE / RMSLE ) the Bayesian approach seems to perform better than the frequentest approach, even in the case of low variability where bias is high (compensated by better precision)
* however current decision schemes for IH is focused on the position of upper confidence limits compared to OEL, and so coverage maybe better suited to evaluate the potential performance in terms of risk decision. In this case, the GUM approach seems to perform better than the Bayesian approach (albeit by a small margin) in the case of using 70% confidence and in the small parameter space studied.

It seems important to get further insight by evalating performnce more directly in terms of sensitivity and specificity, across a spectrum of trues values of exceedance, which require a more complex simulation setup.


in this case the GUM approach seems to perform better than the Bayesian approach, especially in the case of low variability.





