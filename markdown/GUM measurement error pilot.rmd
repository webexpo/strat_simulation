---
title: "GUM measurement error pilot"
author: "Jérôme Lavoué"
date: "2024-06-19"
output: word_document
---

```{r setup, include=FALSE}


knitr::opts_chunk$set(echo = TRUE)

knitr::opts_chunk$set(warning = FALSE, message = FALSE)

library(readxl)
library(ggplot2)
library(flextable)

results <- readRDS("../created data/GUM measurement error.RDS")

example <- readRDS( "../created data/GUM measurement error_ex1.RDS")
          

```

## Introduction

This document summarizes a short  simulation effort aiming at illustrating 2 approaches to deal with measurement error in the statistical evaluation of measurement data. 

True exposure levels to airborne chemicals are often assumed to reasonably follow the lognormal distribution. When performing risk assessment, best practive recommend collecting 6-12 samples from the workplace to estimate metrics such as the 95th percentile or exceedance fraction of the OEL. However, the true exposure the measurements are subject to error potentially introduced during the sampling and analysis phases of the measurement process.

Sampling and analysis (S&A) errors are often assumed to be normally distributed with mean zero.

Two landmark documents have studied the impact of measurement error on the statistical evaluation of exposure data. [Grzebyk and sandino](https://www.inrs.fr/media.html?refINRS=ND%202231) in France and [Nicas, Simmons and Spear](https://www.tandfonline.com/doi/abs/10.1080/15298669191365199) in the US used slightly different approximations to demonstrate that S&A error below a CV of 30% has a negligible impact on the final estimate of the exposure metric for typical workplace GSD.

Both teams had to use simplifying assumptions to make their conclusions. In recent decades advances in computing power have let to the rapid spread of Bayesian methods in industrial hygiene and other fields. Bayesian methods are very flexible, and indeed can be easily used to incorporate S&A error in the interpretation of exposure levels.

An Bayesian S&A error model has been developed by our team at University of Montreal during the [Webexpo project](https://www.irsst.qc.ca/en/publications-tools/publication/i/101066/n/webexpo). 

In parallel, the [supplement 1 to the ISO Guide to the expression of uncertainty in measurement (GUM)](https://www.iso.org/standard/50462.html) provides a framework to propagate measurement error in the final estimate of a statistical parameter using Monte Carlo simulation. 

This document will compare the performance of the Bayesian model to the GUM model for industrial hygiene data interpretation in the context of a simple simulation study.

## Webexpo Bayesian S&A error model

The model is described in details in the scientific reports linked above.

In essence: 

true_concentration is what really happens in the workplace, it is assumed to be lognormal with unknown geometric mean (GM) and geometric standard deviation (GSD).

Mathematically :  

* ln(true_concentration) ~ Normal( ln(GM ) , ln(GSD) )

observed_concentration is what comes out of the laboratory. For a particular value of true_concentration, the observed_concentration is assumed to be equal to the true value plus an error term, which is traditionally assumed to be normally distributed with mean 0 and a known coefficient of variation (CV). 

Mathematically : 

* observed_concentration = true_concentration + error

* error ~ Normal(0,CV)
                
As an example, for an expanded uncertainty of 50%, the CV would be ~25%.

During the analysis of a sample with this model, one would supply the observed values as well as the assumed CV, and the Bayesian engine would return posterior samples for all metrics of interest. The posterior samples represent the estimated uncertainty distribution of the parameter of interest. It is typical to use the median of the posterior sample as the point estimate, and, e.g., the 70% quantile of the posterior sample as a 70% upper confidence limit.

## GUM approach

This is the approach described by email by Robert Emond in an email from June 6th, 202 as understood (possibly misunderstood) by Jérôme Lavoué

The steps to estimate parameters and or upper limits is as follows: 

    
* sample from a normal distribution around the observed values
* calculate a metric with the new dataset
* repeat a lot of times - result is a lot of values of the metric
* report the average value of the metric / check that the variability across repeats is reasonable
    

For the practical implementation of the GUM approach, I used the frequentist estimation methods for the 95th percentile as presented in . To be able to obtain the 70% UCL I used tables in Zwillinger, D.; Kokoska, S. (2000) Standard Probability and Statistics Tables and Formulae. Chapman & Hall / CRC, Boca Raton, FL
Pp 176 (section 7.3 Tolerance factors for the normal distribution)
    
## Example using both approaches

The vector of true concentrations is : `r signif(example$true_data,2)`

It was generated from a lognormal distribution with 95th percentile=100 and GSD=2.5

The vector of observed concentration was created by adding normal random noise to the true concentrations. The observed concentrations would be what is accessible to the industrial hygienist. We selected a value for S&E error of 25%, which correspond to an expanded uncertainty of 50%.

The vector of observed concentrations is : `r signif(example$observed_data,2)`

Table 1 below shows the results of the determination of 4 metrics : gm, gsd, 95th percentile (p95) and the 70% upper confidence limit on the 95th percentile (P95_ucl). These were calculated using both a Bayesian and Frequentist framework for three conditions : 

* Ideal condition : the analysis was performed on the true concentrations, i.e, if there was no measurement error at all
* Naive condition : the analysis was performed on the observed concentrations, but without any provision for measurement error in the estimation process. This represents what is done by most tools today.
* ME condition :  the analysis was performed on the observed concentrations, this time with provision for measurement error: The Bayesian measurement error model for the Bayesian analysis and the GUM approach for the Frequentist analysis.

```{r table 1, warning=FALSE}


T1 <- example$results[-1,]

ft <- flextable(T1)

ft <- autofit(ft)

ft <- align( ft, j = 2:5, align = "center", part = "all" )

ft <- set_caption(ft, "Table 1 : Applying 2 measurement error approaches to one sample", style = "strong")

ft <- colformat_double(ft, j = c(2:5), digits = 1)

ft <- footnote( ft ,
                i = 1 ,
                j = 1 ,
                value = as_paragraph("Ideal : analysis performed on true concentrations with standard approach, Naive : analysis performed on observed concentrations with standard approach, me : analysis performed on observed concentrations with measurement error approach (Bayesian or GUM)"),
                ref_symbols = "a", part = "header")


ft <- footnote( ft ,
                i = 1 ,
                j = 1 ,
                value = as_paragraph("_b : Bayesian method, _f : frequentist method"),
                ref_symbols = "b", part = "header")


ft

```

Observations from T1 : 

* GM seems little impacted
* The naive approach clearly shows an overestimation of environmental variability (expected as we observed environmental variability as well as S&E noise )
* For GSD, the frequentist approach considerably increase the estimate compared to naive approach (expected since normal noise is added to the observations in the Monte Carlo procedure)
* For GSD, the Bayesian approach decreases the estimate compared to naive approach (expected since the Bayesian model takes the S&E noise into account when trying to estimate the variability of the true concentrations)

* Overall the Bayesian ME approach seems to get results closer to the ideal results compared to the GUM approach, but not always much closer.    
    


## limited simulation study
    
    
The previous example illustrate the general behaviour of both measurement error approaches, but doesn't really provide useful information as to their performance / interest.

TO get a little bit more insight we performed a simulation study for 4 scenarios of sample size and true variability : GSD = 1.5 or 2.5 , n=3 or 6. We ketp the true 95th percentile at 100. For each scenario we generated 1000 datasets of true and observed concentrations using an expanded S&E uncertainty CV of 50%. For each simulated dataset we calculated the same metrics as shown above.

To summarize the results across the 10 000 we calculated, for gm, gsd and P95 the bias, precision and rmse of the estimates. For the 70% UCL on the 95th percentile we calculated the coverage of that UCL i.e. the proportion of time the calculated UCL was indeed greater than the true P95 of 100.

The stability of the simulation was assessed by looking at the variability of the results across the 5 repeats of te entire simulation effort. To limit the amount of information presented, we only show the mean results across the 5 repetitions as the standard deviation across the 5 repetitions were very small. They are however available in the raw data.

Overall this took approximate 20h of computing time on a modern desktop computer.

### results

#### geometric mean

Table 2 below presents the results for the estimation of the geometric mean. All metrics are relative to the true value of the parameter. 


```{r table 2, warning=FALSE}


T2 <- results$bias$n6_2.5[,1:2]


#BIAS
T2 <- cbind(T2, results$bias$n3_2.5[,2],
                results$bias$n6_1.5[,2],
                results$bias$n3_1.5[,2])

#PRECISION
T2 <- cbind(T2, results$precision$n6_2.5[,2],
                results$precision$n3_2.5[,2],
                results$precision$n6_1.5[,2],
                results$precision$n3_1.5[,2])

#RMSE
T2 <- cbind(T2, results$rmse$n6_2.5[,2],
                results$rmse$n3_2.5[,2],
                results$rmse$n6_1.5[,2],
                results$rmse$n3_1.5[,2])


colnames(T2) <- c("Approach","bias_n6_2.5","bias_n3_2.5","bias_n6_1.5","bias_n3_1.5",
                  "precision_n6_2.5","precision_n3_2.5","precision_n6_1.5","precision_n3_1.5",
                  "rmse_n6_2.5","rmse_n3_2.5","rmse_n6_1.5","rmse_n3_1.5")

ft <- flextable(T2)

ft <- autofit(ft)


## FIRST LINES

ft <- set_header_labels( ft, 
                            values = list(Approach = "Approach",
                                          bias_n6_2.5 = "n=6",
                                          bias_n3_2.5 = "n=3",
                                          bias_n6_1.5 = "n=6",
                                          bias_n3_1.5 = "n=3",
                                          precision_n6_2.5 = "n=6",
                                          precision_n3_2.5 = "n=3",
                                          precision_n6_1.5 = "n=6",
                                          precision_n3_1.5 = "n=3",
                                          rmse_n6_2.5 = "n=6",
                                          rmse_n3_2.5 = "n=3",
                                          rmse_n6_1.5 = "n=6",
                                          rmse_n3_1.5 = "n=3"))

                                          
#gsd                                          
ft <- add_header_row( ft , top = TRUE, 
                          values = c("", rep(rep(c("GSD=2.5","GSD=1.5"),c(2,2)),3)  ))

ft <- merge_at( ft , i = 1, j = c(2:3), part = "header")
ft <- merge_at( ft , i = 1, j = c(4:5), part = "header")
ft <- merge_at( ft , i = 1, j = c(6:7), part = "header")
ft <- merge_at( ft , i = 1, j = c(8:9), part = "header")
ft <- merge_at( ft , i = 1, j = c(10:11), part = "header")
ft <- merge_at( ft , i = 1, j = c(12:13), part = "header")

# metric
ft <- add_header_row( ft , top = TRUE, 
                          values = c("", rep(c("Bias","Precision","RMSE"),c(4,4,4))))

    
ft <- merge_at( ft , i = 1, j = c(2:5), part = "header")
ft <- merge_at( ft , i = 1, j = c(6:9), part = "header")
ft <- merge_at( ft , i = 1, j = c(10:13), part = "header")
    
        
##  rest


ft <- align( ft, j = 2:13, align = "center", part = "all" )

ft <- set_caption(ft, "Table 2 : Performance metrics for GM", style = "strong")

ft <- colformat_double(ft, j = c(2:13), digits = 1)

ft <- footnote( ft ,
                i = 1 ,
                j = c(2,6,10) ,
                value = as_paragraph("all metrics are relative to the true value of the parameter and expressed in %"),
                ref_symbols = "a", part = "header")


ft <- vline(ft, i = NULL , j= c(1,5,9), border = NULL, part = "body")
    

ft

```


#### geometric standard deviation

Table 3 below presents the results for the estimation of the geometric standard deviation. All metrics are relative to the true value of the parameter. 


```{r table 3, warning=FALSE}


T3 <- results$bias$n6_2.5[,c(1,4)]


#BIAS
T3 <- cbind(T3, results$bias$n3_2.5[,4],
                results$bias$n6_1.5[,4],
                results$bias$n3_1.5[,4])

#PRECISION
T3 <- cbind(T3, results$precision$n6_2.5[,4],
                results$precision$n3_2.5[,4],
                results$precision$n6_1.5[,4],
                results$precision$n3_1.5[,4])

#RMSE
T3 <- cbind(T3, results$rmse$n6_2.5[,4],
                results$rmse$n3_2.5[,4],
                results$rmse$n6_1.5[,4],
                results$rmse$n3_1.5[,4])


colnames(T3) <- c("Approach","bias_n6_2.5","bias_n3_2.5","bias_n6_1.5","bias_n3_1.5",
                  "precision_n6_2.5","precision_n3_2.5","precision_n6_1.5","precision_n3_1.5",
                  "rmse_n6_2.5","rmse_n3_2.5","rmse_n6_1.5","rmse_n3_1.5")

ft <- flextable(T3)

ft <- autofit(ft)


## FIRST LINES

ft <- set_header_labels( ft, 
                            values = list(Approach = "Approach",
                                          bias_n6_2.5 = "n=6",
                                          bias_n3_2.5 = "n=3",
                                          bias_n6_1.5 = "n=6",
                                          bias_n3_1.5 = "n=3",
                                          precision_n6_2.5 = "n=6",
                                          precision_n3_2.5 = "n=3",
                                          precision_n6_1.5 = "n=6",
                                          precision_n3_1.5 = "n=3",
                                          rmse_n6_2.5 = "n=6",
                                          rmse_n3_2.5 = "n=3",
                                          rmse_n6_1.5 = "n=6",
                                          rmse_n3_1.5 = "n=3"))

                                          
#gsd                                          
ft <- add_header_row( ft , top = TRUE, 
                          values = c("", rep(rep(c("GSD=2.5","GSD=1.5"),c(2,2)),3)  ))

ft <- merge_at( ft , i = 1, j = c(2:3), part = "header")
ft <- merge_at( ft , i = 1, j = c(4:5), part = "header")
ft <- merge_at( ft , i = 1, j = c(6:7), part = "header")
ft <- merge_at( ft , i = 1, j = c(8:9), part = "header")
ft <- merge_at( ft , i = 1, j = c(10:11), part = "header")
ft <- merge_at( ft , i = 1, j = c(12:13), part = "header")

# metric
ft <- add_header_row( ft , top = TRUE, 
                          values = c("", rep(c("Bias","Precision","RMSE"),c(4,4,4))))

    
ft <- merge_at( ft , i = 1, j = c(2:5), part = "header")
ft <- merge_at( ft , i = 1, j = c(6:9), part = "header")
ft <- merge_at( ft , i = 1, j = c(10:13), part = "header")
    
        
##  rest


ft <- align( ft, j = 2:13, align = "center", part = "all" )

ft <- set_caption(ft, "Table 3 : Performance metrics for GSD", style = "strong")

ft <- colformat_double(ft, j = c(2:13), digits = 1)

ft <- footnote( ft ,
                i = 1 ,
                j = c(2,6,10) ,
                value = as_paragraph("all metrics are relative to the true value of the parameter and expressed in %"),
                ref_symbols = "a", part = "header")


ft <- vline(ft, i = NULL , j= c(1,5,9), border = NULL, part = "body")
    

ft

```




#### 95th percentile

Table 4 below presents the results for the estimation of the geometric standard deviation. All metrics are relative to the true value of the parameter. 


```{r table 4, warning=FALSE}


T4 <- results$bias$n6_2.5[,c(1,6)]


#BIAS
T4 <- cbind(T4, results$bias$n3_2.5[,6],
                results$bias$n6_1.5[,6],
                results$bias$n3_1.5[,6])

#PRECISION
T4 <- cbind(T4, results$precision$n6_2.5[,6],
                results$precision$n3_2.5[,6],
                results$precision$n6_1.5[,6],
                results$precision$n3_1.5[,6])

#RMSE
T4 <- cbind(T4, results$rmse$n6_2.5[,6],
                results$rmse$n3_2.5[,6],
                results$rmse$n6_1.5[,6],
                results$rmse$n3_1.5[,6])


colnames(T4) <- c("Approach","bias_n6_2.5","bias_n3_2.5","bias_n6_1.5","bias_n3_1.5",
                  "precision_n6_2.5","precision_n3_2.5","precision_n6_1.5","precision_n3_1.5",
                  "rmse_n6_2.5","rmse_n3_2.5","rmse_n6_1.5","rmse_n3_1.5")

ft <- flextable(T4)

ft <- autofit(ft)


## FIRST LINES

ft <- set_header_labels( ft, 
                            values = list(Approach = "Approach",
                                          bias_n6_2.5 = "n=6",
                                          bias_n3_2.5 = "n=3",
                                          bias_n6_1.5 = "n=6",
                                          bias_n3_1.5 = "n=3",
                                          precision_n6_2.5 = "n=6",
                                          precision_n3_2.5 = "n=3",
                                          precision_n6_1.5 = "n=6",
                                          precision_n3_1.5 = "n=3",
                                          rmse_n6_2.5 = "n=6",
                                          rmse_n3_2.5 = "n=3",
                                          rmse_n6_1.5 = "n=6",
                                          rmse_n3_1.5 = "n=3"))

                                          
#gsd                                          
ft <- add_header_row( ft , top = TRUE, 
                          values = c("", rep(rep(c("GSD=2.5","GSD=1.5"),c(2,2)),3)  ))

ft <- merge_at( ft , i = 1, j = c(2:3), part = "header")
ft <- merge_at( ft , i = 1, j = c(4:5), part = "header")
ft <- merge_at( ft , i = 1, j = c(6:7), part = "header")
ft <- merge_at( ft , i = 1, j = c(8:9), part = "header")
ft <- merge_at( ft , i = 1, j = c(10:11), part = "header")
ft <- merge_at( ft , i = 1, j = c(12:13), part = "header")

# metric
ft <- add_header_row( ft , top = TRUE, 
                          values = c("", rep(c("Bias","Precision","RMSE"),c(4,4,4))))

    
ft <- merge_at( ft , i = 1, j = c(2:5), part = "header")
ft <- merge_at( ft , i = 1, j = c(6:9), part = "header")
ft <- merge_at( ft , i = 1, j = c(10:13), part = "header")
    
        
##  rest


ft <- align( ft, j = 2:13, align = "center", part = "all" )

ft <- set_caption(ft, "Table 4 : Performance metrics for the 95th percentile", style = "strong")

ft <- colformat_double(ft, j = c(2:13), digits = 1)

ft <- footnote( ft ,
                i = 1 ,
                j = c(2,6,10) ,
                value = as_paragraph("all metrics are relative to the true value of the parameter and expressed in %"),
                ref_symbols = "a", part = "header")


ft <- vline(ft, i = NULL , j= c(1,5,9), border = NULL, part = "body")
    

ft

```




#### 95th percentile UCL

Table 5 below presents the results for the coverage of the calculated 70%UCL : ideally it should be superioror equal to 70%: the 70% UCL should be above the true value at least 70% of the time. 


```{r table 5, warning=FALSE}


T4 <- results$coverage[,c(1,2,4,6,8)]




colnames(T4) <- c("Approach","n6_2.5","n3_2.5","n6_1.5","n3_1.5")

ft <- flextable(T4)

ft <- autofit(ft)


## FIRST LINES

ft <- set_header_labels( ft, 
                            values = list(Approach = "Approach",
                                          n6_2.5 = "n=6",
                                          n3_2.5 = "n=3",
                                          n6_1.5 = "n=6",
                                          n3_1.5 = "n=3"))

                                          
#gsd                                          
ft <- add_header_row( ft , top = TRUE, 
                          values = c("", rep(c("GSD=2.5","GSD=1.5"),c(2,2))  ))

ft <- merge_at( ft , i = 1, j = c(2:3), part = "header")
ft <- merge_at( ft , i = 1, j = c(4:5), part = "header")



##  rest


ft <- align( ft, j = 2:5, align = "center", part = "all" )

ft <- set_caption(ft, "Table 5 : Coverage the 95th percentile 70% UCL (%)", style = "strong")

ft <- colformat_double(ft, j = c(2:5), digits = 0)

ft <- vline(ft, i = NULL , j= c(1,3), border = NULL, part = "body")
    

ft

```



